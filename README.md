This documentation describes how to setup the ELK stack and the Apache access log generator within AWS.

# Installation of Logstash and the required plugins

Launch an EC2 instance with Linux AMI and SSH to perform the following

$ java -version  
$ sudo yum remove java-1.7.0-openjdk -y

$ sudo yum install java-1.8.0 -y   
$ sudo nano /etc/yum.repos.d/logstash.repo  
```
[logstash-6.x]
name=Elastic repository for 6.x packages
baseurl=https://artifacts.elastic.co/packages/6.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
```
$ sudo yum install logstash -y  

# Installation of Amazon Output filter plugins for logstash 

$ cd /usr/share/logstash/  
$ ll  
$ sudo bin/logstash-plugin install logstash-output-amazon_es  

# logstash configuration

$ sudo nano /etc/logstash/conf.d/logstash.conf  
```
input {
    file {
       path => "/var/log/apache/*"
    }
}

filter {
  date {
    match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
    locale => en
    remove_field => ["timestamp"]
  }
  geoip {
    source => "clientip"
  }
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
    remove_field => "message"
  }
  useragent {
    source => "agent"
    target => "useragent"
  }
}

output {
    amazon_es {
      hosts => "<AWS-ES-DOMAIN-ENTPOINT-URL>"
      port => "443"
      region => "<AWS-REGION-ID>"
      index => "demolog"
      document_type => "apache"
      manage_template => false
    }
}
```
 
# Installation of "Fake Apache Log Generation" for apache log generation

use the generator available <https://github.com/kiritbasu/Fake-Apache-Log-Generator>

$ sudo yum install git -y  
$ git clone https://github.com/kiritbasu/Fake-Apache-Log-Generator.git  
$ cd Fake-Apache-Log-Generator/   
$ sudo pip install -r requirements.txt  
$ python apache-fake-log-gen.py  
$ sudo mkdir -p /var/log/apache/  
$ sudo python apache-fake-log-gen.py -n 0 -s 1 -o LOG -p /var/log/apache/ &  
$ cd /var/log/apache/  

### Start and Shutdown Logstash

$ sudo bin/logstash -f /etc/logstash/conf.d/logstash.conf

$ ps -ef | grep logstash  
$ sudo kill <process-id> 

# Setup an AWS Elasticsearch Domain

Setup an AWS Elasticsearch domain either from the AWS Management Console or using aws-cli command below.

Required parameters:
```
<aws-region-id>
<aws-account-id>
<public-IP-address-of-your-machine>
```
```
% aws es create-elasticsearch-domain --domain-name demo-es --elasticsearch-version 6.2 --elasticsearch-cluster-config InstanceType=t2.micro.elasticsearch,InstanceCount=1 --access-policies '{"Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Principal": {"AWS": "*"}, "Action":"es:*", "Resource": "arn:aws:es:<aws-region-id>:<aws-account-id>:domain/demo-es/*", "Condition": { "IpAddress": { "aws:SourceIp": "<public-IP-address-of-your-machine>" } } } ] }' --region <aws-region-id>
```

Once created and the status is shown as Active, then check whether you can access the Kibana URL in your local browser.

Confirm that the “demolog” has been created in Elasticsearch
```
GET _cat/indices?v
```

# Modify the auto-created index mapping to create an index template with appropriate field types
Create an index pattern for the “demolog” index in Kibana by nananogating to Management —> Index Patterns

Check the field types for the geo.* fields that were generated by Logstash

Download the index mapping into the local machine and modify the below field types accordingly
```
% curl -XGET '<AWS-ES-DOMAIN-ENTPOINT-URL>/demolog/_mapping?pretty' > demolog_mapping.json
```
```
"dma_code" : { "type" : "short" },
"ip" : { "type" : "ip" },
"latitude" : { "type" : "half_float" },
"location" : { "type" : "geo_point" },
"longitude" : { "type" : "half_float" },
```
```
"bytes" : { "type" : "long" },
```
Create a new file and Save As “demolog_template.json”

Remove the first two lines and add the followings
```
{
  "template": "demolog",
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
 ```
Remove the last '}'

Save the modified template file and upload it.

% curl -XPUT '<AWS-ES-DOMAIN-ENTPOINT-URL>/_template/demolog?pretty' -H 'Content-Type: application/json' -d @demolog_template.json

Confirm the template creation in Elasticsearch

GET _template/demolog?pretty

Finally, go ahead delete the existing index from Elasticsearch and observe how the index is created following the uploaded template.
```
DELETE demolog?pretty
GET _cat/indices?v
GET demolog?pretty
```


# Cleanup
```
% aws ec2 terminate-instances --instance-ids <value> --region-name <aws-region-id>
% aws es delete-elasticsearch-domain --domain-name demo-es --region-name <aws-region-id>
```
# References

https://www.elastic.co/guide/en/elasticsearch/reference/5.2/rpm.html
https://github.com/awslabs/logstash-output-amazon_es
https://www.elastic.co/guide/en/logstash/current/installing-logstash.html
https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-createupdatedomains.html#es-createdomains-configure-cluster
https://www.elastic.co/webinars/getting-started-kibana
https://www.elastic.co/guide/index.html
https://www.elastic.co/guide/en/logstash/current/filter-plugins.html
https://github.com/kiritbasu/Fake-Apache-Log-Generator

